{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5de203d-d412-46ec-a755-776f0d0144c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 02:40:06.251200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-23 02:40:12 [INFO] Dataset loaded → 44898 rows (fake=23481  real=21417)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 02:44:30 [INFO] Epoch 1   val_F1=0.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 238s - 2s/step - accuracy: 0.9464 - loss: 0.1425 - precision: 0.9549 - recall: 0.9316 - val_accuracy: 0.9906 - val_loss: 0.0395 - val_precision: 0.9967 - val_recall: 0.9835 - val_f1: 0.9900\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 02:48:27 [INFO] Epoch 2   val_F1=0.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 237s - 2s/step - accuracy: 0.9970 - loss: 0.0177 - precision: 0.9968 - recall: 0.9969 - val_accuracy: 0.9958 - val_loss: 0.0234 - val_precision: 0.9972 - val_recall: 0.9940 - val_f1: 0.9956\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 02:52:09 [INFO] Epoch 3   val_F1=0.9928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 222s - 2s/step - accuracy: 0.9994 - loss: 0.0091 - precision: 0.9995 - recall: 0.9992 - val_accuracy: 0.9932 - val_loss: 0.0327 - val_precision: 0.9983 - val_recall: 0.9874 - val_f1: 0.9928\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 02:55:47 [INFO] Epoch 4   val_F1=0.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 219s - 2s/step - accuracy: 0.9993 - loss: 0.0077 - precision: 0.9995 - recall: 0.9990 - val_accuracy: 0.9958 - val_loss: 0.0254 - val_precision: 0.9972 - val_recall: 0.9940 - val_f1: 0.9956\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 02:59:27 [INFO] Epoch 5   val_F1=0.9962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 219s - 2s/step - accuracy: 0.9999 - loss: 0.0045 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9963 - val_loss: 0.0228 - val_precision: 0.9972 - val_recall: 0.9951 - val_f1: 0.9962\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 03:03:07 [INFO] Epoch 6   val_F1=0.9964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 221s - 2s/step - accuracy: 1.0000 - loss: 0.0034 - precision: 1.0000 - recall: 0.9999 - val_accuracy: 0.9966 - val_loss: 0.0211 - val_precision: 0.9972 - val_recall: 0.9956 - val_f1: 0.9964\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 03:06:47 [INFO] Epoch 7   val_F1=0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 220s - 2s/step - accuracy: 1.0000 - loss: 0.0026 - precision: 1.0000 - recall: 0.9999 - val_accuracy: 0.9961 - val_loss: 0.0243 - val_precision: 0.9972 - val_recall: 0.9945 - val_f1: 0.9959\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 03:10:41 [INFO] Epoch 8   val_F1=0.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 234s - 2s/step - accuracy: 1.0000 - loss: 0.0021 - precision: 1.0000 - recall: 0.9999 - val_accuracy: 0.9958 - val_loss: 0.0239 - val_precision: 0.9972 - val_recall: 0.9940 - val_f1: 0.9956\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 03:14:22 [INFO] Epoch 9   val_F1=0.9953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 - 221s - 2s/step - accuracy: 1.0000 - loss: 0.0017 - precision: 1.0000 - recall: 0.9999 - val_accuracy: 0.9955 - val_loss: 0.0249 - val_precision: 0.9967 - val_recall: 0.9940 - val_f1: 0.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 03:14:33 [INFO] TEST  loss=0.0167  acc=0.9963  prec=0.9972  rec=0.9950  f1=0.9961\n",
      "2025-04-23 03:14:44 [INFO] Dashboard → artifacts/plots/training_dashboard.html\n",
      "2025-04-23 03:14:45 [INFO] Confusion matrix → artifacts/plots/confusion_matrix.html\n",
      "2025-04-23 03:14:52 [INFO] Word-cloud → artifacts/plots/wc_fake.png\n",
      "2025-04-23 03:14:58 [INFO] Word-cloud → artifacts/plots/wc_real.png\n",
      "2025-04-23 03:14:59 [INFO] All artifacts saved under /Users/abhijitsinha/Desktop/Fake News Detection/Notebook/artifacts\n"
     ]
    }
   ],
   "source": [
    "# ❶ Imports\n",
    "# ----------------------------------------------------------------------\n",
    "from __future__ import annotations\n",
    "import os, argparse, logging, pathlib, string\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "from bokeh.plotting import figure, save as bokeh_save, output_file\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import ColumnDataSource, ColorBar, LinearColorMapper\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❷ Config  (edit the two CSV paths to suit your machine) \n",
    "# ----------------------------------------------------------------------\n",
    "DATA_FAKE_PATH = \"/Users/abhijitsinha/Desktop/Fake News Detection/Data/Fake.csv\"   # <- change if needed\n",
    "DATA_TRUE_PATH = \"/Users/abhijitsinha/Desktop/Fake News Detection/Data/True.csv\"   # <- change if needed\n",
    "\n",
    "VOCAB_SIZE, MAX_SEQUENCE_LENGTH  = 40_000, 300\n",
    "EMBED_DIM,  LSTM_UNITS,  DROPOUT = 128, 128, 0.30\n",
    "BATCH_SIZE, EPOCHS               = 256, 15\n",
    "LEARNING_RATE                    = 2e-4     # ← ASCII minus\n",
    "VALID_SPLIT, TEST_SPLIT          = 0.10, 0.15\n",
    "ARTIFACT_DIR                     = pathlib.Path(\"artifacts\")\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❸ Logging & TensorFlow one-time setup\n",
    "# ----------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"                  # hide TF C++ spam\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):        # polite VRAM usage\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Could not set GPU memory-growth: %s\", e)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❹ Data utilities\n",
    "# ----------------------------------------------------------------------\n",
    "def load_and_combine(fake_csv: str, true_csv: str) -> pd.DataFrame:\n",
    "    fake, true = pd.read_csv(fake_csv), pd.read_csv(true_csv)\n",
    "    fake[\"target\"], true[\"target\"] = 0, 1\n",
    "    df = (pd.concat([fake, true], ignore_index=True)\n",
    "            .sample(frac=1.0, random_state=42)\n",
    "            .reset_index(drop=True))\n",
    "    logging.info(\"Dataset loaded → %d rows (fake=%d  real=%d)\",\n",
    "                 len(df), len(fake), len(true))\n",
    "    return df\n",
    "\n",
    "def _clean(text: str) -> str:\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return \" \".join([t for t in text.split() if t not in STOPWORDS])\n",
    "\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(\"Column 'text' not found in dataset.\")\n",
    "    df = df.copy()\n",
    "    df[\"text\"] = df[\"text\"].astype(str).apply(_clean)\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❺ Tokenisation helpers\n",
    "# ----------------------------------------------------------------------\n",
    "def build_tokenizer(texts: pd.Series) -> tf.keras.preprocessing.text.Tokenizer:\n",
    "    tok = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE,\n",
    "                                                oov_token=\"<UNK>\")\n",
    "    tok.fit_on_texts(texts)\n",
    "    return tok\n",
    "\n",
    "def vectorise(tok, texts: pd.Series) -> np.ndarray:\n",
    "    seqs = tok.texts_to_sequences(texts)\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        seqs, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\"\n",
    "    )\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❻ Model\n",
    "# ----------------------------------------------------------------------\n",
    "def make_model(vocab_size: int) -> tf.keras.Model:\n",
    "    inp = layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    x   = layers.Embedding(vocab_size, EMBED_DIM, mask_zero=True)(inp)\n",
    "    x   = layers.Dropout(DROPOUT)(x)\n",
    "    x   = layers.Bidirectional(layers.LSTM(LSTM_UNITS))(x)\n",
    "    x   = layers.LayerNormalization()(x)\n",
    "    x   = layers.Dropout(DROPOUT)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\",\n",
    "                 tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                 tf.keras.metrics.Recall(name=\"recall\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❼ Custom callback for F1\n",
    "# ----------------------------------------------------------------------\n",
    "class F1Callback(callbacks.Callback):\n",
    "    def __init__(self, val_xy): super().__init__(); self.val_x, self.val_y = val_xy; self.f1=[]\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        preds = (self.model.predict(self.val_x, verbose=0) > .5).astype(int)\n",
    "        p,r,f1,_ = precision_recall_fscore_support(self.val_y, preds,\n",
    "                                                   average=\"binary\", zero_division=0)\n",
    "        self.f1.append(f1); logs |= {}; logs.update(val_precision=p,val_recall=r,val_f1=f1)\n",
    "        logging.info(\"Epoch %-2d  val_F1=%.4f\", epoch+1, f1)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❽ Bokeh dashboards\n",
    "# ----------------------------------------------------------------------\n",
    "def save_training_dashboard(hist: callbacks.History, f1: List[float], out: pathlib.Path):\n",
    "    epochs = range(1, len(hist.history[\"loss\"])+1)\n",
    "    src = ColumnDataSource(dict(epoch=list(epochs),\n",
    "                                loss=hist.history[\"loss\"],\n",
    "                                val_loss=hist.history[\"val_loss\"],\n",
    "                                acc=hist.history[\"accuracy\"],\n",
    "                                val_acc=hist.history[\"val_accuracy\"],\n",
    "                                f1=f1))\n",
    "    p_loss = figure(title=\"Loss\", x_axis_label=\"Epoch\",\n",
    "                    y_axis_label=\"BCE\", width=400, height=300)\n",
    "    p_loss.line(\"epoch\", \"loss\",     source=src, legend_label=\"train\")\n",
    "    p_loss.line(\"epoch\", \"val_loss\", source=src, color=\"green\", legend_label=\"val\")\n",
    "\n",
    "    p_acc = figure(title=\"Accuracy\", x_axis_label=\"Epoch\",\n",
    "                   y_axis_label=\"Acc\", width=400, height=300)\n",
    "    p_acc.line(\"epoch\", \"acc\",     source=src, legend_label=\"train\")\n",
    "    p_acc.line(\"epoch\", \"val_acc\", source=src, color=\"green\", legend_label=\"val\")\n",
    "\n",
    "    p_f1  = figure(title=\"F1\", x_axis_label=\"Epoch\",\n",
    "                   y_axis_label=\"F1\", width=400, height=300)\n",
    "    p_f1.line(\"epoch\", \"f1\", source=src, color=\"red\")\n",
    "\n",
    "    dashboard = gridplot([[p_loss, p_acc, p_f1]])\n",
    "    output_file(out); bokeh_save(dashboard); logging.info(\"Dashboard → %s\", out)\n",
    "\n",
    "def save_confusion(cm: np.ndarray, out: pathlib.Path):\n",
    "    cats = [\"Fake\", \"Real\"]\n",
    "    mapper = LinearColorMapper(\"Viridis256\", low=0, high=int(cm.max()))\n",
    "    x,y = np.meshgrid(range(2), range(2))\n",
    "    src = ColumnDataSource(dict(x=x.ravel(), y=y.ravel(), val=cm.ravel()))\n",
    "    p = figure(x_range=cats, y_range=list(reversed(cats)), width=350, height=350,\n",
    "               toolbar_location=None, title=\"Confusion Matrix\")\n",
    "    p.rect(x=\"x\", y=\"y\", width=1, height=1, source=src,\n",
    "           fill_color={'field':'val','transform':mapper}, line_color=None)\n",
    "    p.add_layout(ColorBar(color_mapper=mapper), \"right\")\n",
    "    p.xaxis.major_label_orientation=\"vertical\"\n",
    "    output_file(out); bokeh_save(p); logging.info(\"Confusion matrix → %s\", out)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❾ Word-clouds\n",
    "# ----------------------------------------------------------------------\n",
    "def wordcloud(text: str, title: str, out: pathlib.Path):\n",
    "    wc = WordCloud(width=1600, height=900, max_font_size=140,\n",
    "                   collocations=False).generate(text)\n",
    "    plt.figure(figsize=(14,8)); plt.imshow(wc); plt.axis(\"off\"); plt.title(title)\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=140); plt.close()\n",
    "    logging.info(\"Word-cloud → %s\", out)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ❿ Main pipeline\n",
    "# ----------------------------------------------------------------------\n",
    "def main(fake_csv=DATA_FAKE_PATH, true_csv=DATA_TRUE_PATH):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "    df    = preprocess(load_and_combine(fake_csv, true_csv))\n",
    "    train_val_x, X_test, train_val_y, y_test = train_test_split(\n",
    "        df[\"text\"], df[\"target\"], test_size=TEST_SPLIT,\n",
    "        stratify=df[\"target\"], random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_val_x, train_val_y, test_size=VALID_SPLIT,\n",
    "        stratify=train_val_y, random_state=42)\n",
    "\n",
    "    tok = build_tokenizer(X_train)\n",
    "    X_train_pad, X_val_pad, X_test_pad = map(\n",
    "        lambda x: vectorise(tok, x), [X_train, X_val, X_test])\n",
    "\n",
    "    train_ds = (tf.data.Dataset.from_tensor_slices((X_train_pad, y_train))\n",
    "                  .shuffle(1024).batch(BATCH_SIZE).prefetch(2))\n",
    "    val_ds   = (tf.data.Dataset.from_tensor_slices((X_val_pad, y_val))\n",
    "                  .batch(BATCH_SIZE).cache().prefetch(2))\n",
    "    test_ds  = (tf.data.Dataset.from_tensor_slices((X_test_pad, y_test))\n",
    "                  .batch(BATCH_SIZE))\n",
    "\n",
    "    model = make_model(VOCAB_SIZE)\n",
    "    cb_early = callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "    cb_tensor= callbacks.TensorBoard(log_dir=ARTIFACT_DIR/\"logs\"/\n",
    "                                     datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    cb_f1    = F1Callback((X_val_pad, y_val))\n",
    "\n",
    "    hist = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS,\n",
    "                     callbacks=[cb_early, cb_tensor, cb_f1], verbose=2)\n",
    "\n",
    "    # Evaluation\n",
    "    loss, acc, prec, rec = model.evaluate(test_ds, verbose=0)\n",
    "    f1 = 2*(prec*rec)/(prec+rec+1e-12)\n",
    "    logging.info(\"TEST  loss=%.4f  acc=%.4f  prec=%.4f  rec=%.4f  f1=%.4f\",\n",
    "                 loss, acc, prec, rec, f1)\n",
    "\n",
    "    # Predictions & confusion matrix\n",
    "    y_pred = (model.predict(test_ds, verbose=0) > .5).astype(int).ravel()\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Save artifacts\n",
    "    # ------------------------------------------------------------------\n",
    "    p_dir = ARTIFACT_DIR / \"plots\"; p_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_training_dashboard(hist, cb_f1.f1, p_dir/\"training_dashboard.html\")\n",
    "    save_confusion(cm, p_dir/\"confusion_matrix.html\")\n",
    "    wordcloud(\" \".join(df[df.target==0].text), \"Fake News\", p_dir/\"wc_fake.png\")\n",
    "    wordcloud(\" \".join(df[df.target==1].text), \"Real News\", p_dir/\"wc_real.png\")\n",
    "\n",
    "    model.save(ARTIFACT_DIR/\"fakenews_lstm.keras\", include_optimizer=False)\n",
    "    with open(ARTIFACT_DIR/\"tokenizer.json\", \"w\") as fp:\n",
    "        fp.write(tok.to_json())\n",
    "    logging.info(\"All artifacts saved under %s\", ARTIFACT_DIR.resolve())\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# ⓫ Run immediately when the cell executes\n",
    "# ----------------------------------------------------------------------\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2775e2c-1dd2-4e77-886e-302a7c8d65e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
